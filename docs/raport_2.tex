%styl klasy z Polskimi Normami oprac. Marcin Wolinski
\documentclass[a4paper,titleauthor]{mwart} 
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} %pakiet do wstawiania grafiki
\usepackage[hyphens]{url} %pakiet do wstawiania linkow
\usepackage{authblk}%pakiet do tworzenia afiliacji
\usepackage{tabularx}%pakiet do tabel
\usepackage[a4paper, left=2cm, right=2cm, top=3cm, bottom=3cm]{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{placeins}%pakiet do kontroli umieszczania obiektow
\usepackage{hyperref}%pakiet do m.in. kolorowania linkow
\usepackage{enumitem}
\usepackage[tablegrid,owncaptions]{vhistory}
\usepackage{minted}

\begin{document}

% \begin{titlepage}
   \begin{center}
       % \vspace*{1cm}

       % \textbf{\Huge APEM  } \\
       %  \vspace{1.0cm}
       \huge{NLP - Projekt} \\

       \vspace{1.0cm}
        \LARGE{\textbf{Porównanie metod Parameter-Efficient Fine-tuning (PEFT) na wybranych zadaniach NLP }}

            
        % \vspace{0.5cm}
        % \textsc{Grupa projektowa \\ {\Large{w składzie}} :}

        
        \vspace{1cm}
        \begin{tabular}{c@{\hskip 3cm}c}
             Maja Zglinicka & Tomasz Lewiński \\
             \small{318860} & \small{323692}
        \end{tabular}
        
   \end{center}
% \end{titlepage} 

\tableofcontents
\section{Dokumentacja wstępna}
\subsection{Problematyka}
\subsubsection{Fine-tuning}

W ostatnich latach modele językowe oparte na architekturach transformerowych, takie jak BERT, RoBERTa, czy GPT, stały się podstawowym narzędziem w przetwarzaniu języka naturalnego. Są one trenowane na ogromnych korpusach tekstu i potrafią reprezentować złożone zależności semantyczne, składniowe i pragmatyczne. Mimo że modele te wykazują wysoką jakość w wielu zadaniach, ich skuteczne wykorzystanie w konkretnych zastosowaniach zwykle wymaga etapu dostrajania, czyli fine-tuningu.

Fine-tuning polega na dalszym trenowaniu modelu bazowego na danych specyficznych dla danego zadania, tak aby model nauczył się wykonywać je z wysoką skutecznością. W klasycznym podejściu dostraja się wszystkie parametry modelu, co oznacza aktualizację setek milionów, a nawet miliardów wag. Pozwala to osiągnąć bardzo dobre wyniki, ponieważ cały model może dopasować się do nowej dystrybucji danych.

Pełne dostrajanie dużych modeli jest jednak bardzo kosztowne obliczeniowo. 
Ze względów takich jak ogromna liczba parametrów, duże zużycie pamięci GPU, brak skalowalności czy wysoki koszt czasowy, pełny fine-tuning jest często nieopłacalny, zwłaszcza w warunkach akademickich lub w mniejszych projektach badawczych.

\subsubsection{PEFT}
Aby zmniejszyć koszty czasowe i pamięciowe, zaproponowano grupę technik określanych jako \\
Parameter-Efficient Fine-Tuning (PEFT). 
W podejściach PEFT zamraża się większość parametrów modelu, a dostrajaniu podlega jedynie niewielka dodatkowa liczba nowych parametrów - zwykle ułamek procenta całego modelu. Pozwala to uzyskać wydajność jakościową zbliżoną do pełnego fine-tuningu przy znacznym ograniczeniu wymagań obliczeniowych.\\

Do najważniejszych metod PEFT należą:
\begin{enumerate}
    \item Adaptery - małe moduły, najczęściej niewielkie sieci dwuwarstwowe, wpinane pomiędzy warstwy transformera. Podczas treningu parametry modelu bazowego są zamrożone, trenowane są wyłącznie parametry adapterów. Pozwala to uzyskać bardzo mały koszt pamięciowy, ponieważ liczba dodatkowych parametrów jest niewielka (zwykle mniej niż 3\% modelu).\\
    
    \item LoRA (Low-Rank Adaptation) - LoRA zakłada, że zmiany wymagane do dostosowania wag modelu do nowego zadania można aproksymować macierzą niskiego rzędu. Oznacza to, że zamiast aktualizować dużą macierz wag $W$, uczymy dwie małe macierze $A$ i $B$, które po przemnożeniu dają zbliżony efekt. Dzięki temu trenowanych parametrów może być kilkadziesiąt razy mniej, niż w przypadku pełnego fine-tuningu.\\

    \item Prefix Tuning - do wejścia modelu dodaje się specjalny prefiks w przestrzeni ukrytej modelu. Model bazowy pozostaje zamrożony, a uczone są jedynie te dodatkowe reprezentacje. Metoda ta jest szczególnie skuteczna w zadaniach generatywnych (seq2seq).\\

    \item Prompt Tuning / Soft Prompting - Prompt tuning polega na dodaniu specjalnych, uczonych tokenów promptu do wejścia modelu, które sterują zachowaniem modelu bez zmiany jego wewnętrznych wag. Jest to ekstremalnie lekka forma PEFT, często wymaga najmniej parametrów ze wszystkich metod.

\end{enumerate}

Metody PEFT stanowią atrakcyjną alternatywę dla pełnego fine-tuningu dużych modeli językowych. Pozwalają osiągnąć wysoką jakość wyników przy znacznie mniejszych wymaganiach pamięciowych i obliczeniowych, a także umożliwiają przechowywanie wielu „dostosowanych wersji” danego modelu w formie małych dodatków zamiast pełnych kopii sieci.


\subsection{Cel projektu}

Celem projektu jest zbadanie efektywności wybranych technik PEFT w kontekście różnych zadań przetwarzania języka naturalnego. W ramach pracy zostaną wybrane co najmniej dwa odmienne typy zadań NLP, obejmujące zarówno klasyfikację, jak i zadanie sekwencyjne typu seq2seq, co pozwoli na ocenę działania metod PEFT w różnych scenariuszach. Projekt zakłada implementację oraz porównanie kilku podejść, takich jak LoRA, Prefix Tuning oraz Adaptery, zastosowanych do modeli bazowych odpowiednich dla danych zadań. Analiza obejmie ocenę jakości modeli, a także porównanie kosztów obliczeniowych, czasu trenowania oraz liczby i rozmiaru parametrów aktualizowanych w poszczególnych metodach. Celem końcowym jest ocena, na ile metody PEFT pozwalają uzyskać wyniki zbliżone do pełnego dostrajania modeli językowych przy jednoczesnym znaczącym zmniejszeniu nakładów obliczeniowych.

\subsection{Wybór zadań NLP}

W projekcie zdecydowano się na realizację dwóch odmiennych typów zadań przetwarzania języka naturalnego: zadania klasyfikacji tekstu oraz zadania sekwencyjnego typu seq2seq. Dobór zadań ma na celu zbadanie efektywności metod PEFT zarówno w kontekście analizy tekstu, jak i jego generowania. W obu przypadkach wybrane zostały zbiory danych powszechnie stosowane w badaniach NLP, co pozwoli na rzetelną ocenę jakości modeli.

\subsubsection{Zadanie klasyfikacyjne}

Do zadania klasyfikacji tekstu wybrano zbiór danych \textbf{AI-GA (Artificial Intelligence Generated Abstracts)}, zawierający 28\,662 przykłady obejmujące tytuł, abstrakt oraz etykietę. Zbiór jest zbalansowany i składa się w 50\% z abstraktów wygenerowanych przez modele językowe (etykieta 1), a w pozostałych 50\% z abstraktów oryginalnych (etykieta 0).  

Zadanie polega na rozróżnieniu, czy dany abstrakt został wygenerowany przez system AI, czy pochodzi od człowieka. Charakter danych stanowi dobre wyzwanie dla modeli encoderowych, a jednocześnie pozwala ocenić skuteczność metod PEFT w klasyfikacji bardziej złożonych struktur semantycznych niż krótkie opinie czy recenzje.

Dane zostaną podzielone na następujące części:
\begin{itemize}
    \item 80\% – zbiór treningowy,
    \item 10\% – zbiór walidacyjny,
    \item 10\% – zbiór testowy.
\end{itemize}

Do ewaluacji modeli zastosowane zostaną metryki:
\begin{itemize}
    \item \textbf{Accuracy},
    \item \textbf{F1-score (macro)}
\end{itemize}

\subsubsection{Zadanie seq2seq}

W zadaniu sekwencyjnym wykorzystany zostanie zbiór danych \textbf{XL-Sum}, zawierający artykuły z serwisu BBC wraz z ich profesjonalnymi streszczeniami w 44 językach. Jest to zbiór powszechnie stosowany w badaniach nad wielojęzycznym streszczaniem tekstu.

Zgodnie z sugestią, aby wyjść poza język angielski i sprawdzić działanie metod w różnych warunkach dostępności danych, do eksperymentów wybrano dwa języki:
\begin{itemize}
    \item \textbf{Hiszpański (Spanish)} – jako przykład języka \textit{high-resource}, dla którego dostępna jest duża liczba par artykuł-streszczenie.
    \item \textbf{Nepalski (Nepali)} – jako przykład języka \textit{low-resource}, reprezentującego mniejszy zasób danych treningowych.
\end{itemize}

Celem zadania jest wygenerowanie streszczenia artykułu na podstawie jego pełnej treści. Takie sformułowanie problemu pozwala ocenić zdolność modeli do przetwarzania sekwencji w różnych językach oraz ich efektywność w warunkach ograniczonej liczby danych (przypadek low-resource).

Do ewaluacji jakości generowanych streszczeń zastosowane zostaną metryki:

\begin{itemize}
    \item \textbf{ROUGE-1},
    \item \textbf{ROUGE-2},
    \item \textbf{ROUGE-L},
\end{itemize}

które mierzą odpowiednio zgodność n-gramową oraz zgodność na poziomie najdłuższego wspólnego podciągu między tekstem wygenerowanym a streszczeniem referencyjnym.


\subsection{Wybór modeli bazowych}

W projekcie planujemy wykorzystać dwa modele bazowe reprezentujące odmienne architektury transformacyjne: BERT oraz T5. Modele zostały dobrane tak, aby odzwierciedlał inny typ zadania NLP oraz umożliwiały rzetelne porównanie metod PEFT w różnych kontekstach obliczeniowych i funkcjonalnych.\\

BERT (Bidirectional Encoder Representations from Transformers) to jedna z najpopularniejszych rodzin modeli typu encoder, zaprojektowanych do zadań analizy i rozumienia tekstu. W projekcie planujemy wykorzystać wariant BERT-base, który zawiera 110 milionów parametrów i stanowi powszechnie stosowany punkt odniesienia w badaniach akademickich.\\

Model BERT do zadania klasyfikacji został przez nas wybrany m.in. ze względu na to, że jest dobrze udokumentowany i szeroko wspierany przez biblioteki HuggingFace Transformers, PEFT i AdapterHub, oraz dlatego, że model ma umiarkowany rozmiar, który pozwala na komfortowe przeprowadzanie eksperymentów na dostępnych GPU (np. Google Colab).\\

Drugim wybranym modelem (do zadania seq2seq) jest \textbf{mT5-small} (Multilingual T5). Jest to wielojęzyczny wariant modelu T5, trenowany na korpusie mC4 obejmującym 101 języków. Podobnie jak T5, opiera się on na architekturze encoder–decoder, co czyni go naturalnym wyborem do zadań generowania sekwencji w środowisku wielojęzycznym. Wariant \textit{small} liczy około 300 milionów parametrów (ze względu na większy słownik w porównaniu do T5), co nadal stanowi rozsądny kompromis dla eksperymentów PEFT.\\

Wybór mT5-small jest podyktowany koniecznością obsługi języków innych niż angielski (w naszym przypadku hiszpańskiego i nepalskiego) oraz chęcią sprawdzenia, jak metody PEFT radzą sobie z adaptacją modelu wielojęzycznego do konkretnych języków docelowych.\\

Połączenie modeli BERT-base (encoder, monolingual/English) i mT5-small (encoder–decoder, multilingual) pozwala objąć dwa kluczowe typy zadań NLP oraz różne scenariusze językowe.

\subsection{Wybór metod PEFT}

W projekcie zdecydowano się na zastosowanie trzech reprezentatywnych metod Parameter-Efficient Fine-Tuning, obejmujących odmienne sposoby wprowadzania dodatkowych parametrów do modeli. Ich dobór pozwoli zbadać wpływ strukturalnych różnic pomiędzy technikami na jakość i koszty trenowania zarówno w zadaniach klasyfikacyjnych, jak i generatywnych.

Wybrane metody to:

\begin{enumerate}
    \item \textbf{Adaptery} – technika polegająca na wprowadzeniu niewielkich modułów sieciowych między warstwy modelu. Dzięki temu możliwe jest trenowanie jedynie dodatkowych parametrów, co pozwala na zachowanie niskich kosztów pamięciowych i stabilnego procesu uczenia.

    \item \textbf{LoRA (Low-Rank Adaptation)} – metoda oparta na dekompozycji wag na macierze niskiego rzędu. Umożliwia efektywne dostrajanie modeli przy minimalnym zwiększeniu liczby uczonych parametrów oraz zachowaniu dobrej jakości wyników w różnych typach zadań.

    \item \textbf{Prefix Tuning} – podejście bazujące na uczonych wektorach prefiksowych dodawanych do reprezentacji ukrytej modelu. Jest ono szczególnie użyteczne w modelach encoder–decoder, dlatego w projekcie zostanie zastosowane przede wszystkim w kontekście zadania seq2seq.
\end{enumerate}

Zestawienie tych trzech metod pozwala na porównanie podejść różniących się stopniem ingerencji w architekturę modelu oraz sposobem wprowadzania dodatkowych informacji, co umożliwi uzyskanie pełniejszego obrazu ich efektywności.



\subsection{Plan eksperymentów}

Plan eksperymentów został przygotowany w taki sposób, aby umożliwić porównanie metod PEFT w sposób kontrolowany i jednorodny dla dwóch wybranych zadań: klasyfikacji tekstu oraz streszczania tekstu. W każdym eksperymencie zastosowane zostaną te same zasady podziału danych, jednakowe metody oceny oraz spójna konfiguracja treningowa, co pozwoli skupić uwagę na różnicach wynikających z użytej techniki dostrajania.

W eksperymentach wykorzystane będą:

\begin{itemize}
    \item \textbf{BERT-base} – model encoderowy dla zadania klasyfikacji,
    \item \textbf{mT5-small} – model encoder–decoder dla zadania seq2seq.
\end{itemize}

Dla każdego modelu wykonane zostaną cztery warianty treningu:

\begin{itemize}
    \item pełny fine-tuning (jako punkt odniesienia),
    \item LoRA,
    \item Adaptery,
    \item Prefix Tuning (dla mT5 również jako metoda szczególnie odpowiednia dla architektury encoder–decoder).
\end{itemize}

Wszystkie konfiguracje treningowe będą prowadzone według wspólnego schematu:

\begin{itemize}
    \item stała liczba epok odpowiednia dla danego modelu,
    \item ten sam optymalizator (AdamW) oraz zbliżone wartości learning rate dostosowane do metody (proces doboru LR zostanie udokumentowany),
    \item identyczny batch size w podstawowym porównaniu, z dodatkową analizą wpływu metod na „budżet obliczeniowy” (możliwość zwiększenia batch size dzięki redukcji parametrów),
    \item ten sam podział danych: 80\% trening, 10\% walidacja, 10\% test.
\end{itemize}

Do oceny wyników zastosowane zostaną metryki właściwe dla danego typu zadania:

\begin{itemize}
    \item \textbf{dla klasyfikacji (BERT)}: accuracy oraz F1-score (macro),
    \item \textbf{dla streszczania (mT5)}: ROUGE-1, ROUGE-2, ROUGE-L.
\end{itemize}

Oprócz jakości predykcji analizowane będą również:

\begin{itemize}
    \item liczba trenowanych parametrów,
    \item całkowity czas trenowania,
    \item stabilność uczenia w przebiegu epok.
\end{itemize}

Pozwoli to określić nie tylko skuteczność metod PEFT, ale również ich praktyczną opłacalność obliczeniową. Analiza końcowa obejmie porównania wewnątrz każdego zadania oraz między zadaniami, co umożliwi ocenę, czy wybrane techniki PEFT zachowują podobną efektywność w różnych typach problemów NLP.

\section{Wstępne wyniki}

\subsection{Przygotowanie danych}
W ramach wstępnych prac przygotowano i przeanalizowano wybrane zbiory danych. Poniżej przedstawiono liczności podzbiorów (treningowego, walidacyjnego i testowego) oraz podstawowe statystyki dotyczące długości tekstów.

\subsubsection{Liczności zbiorów}
Zbiory zostały podzielone w standardowych proporcjach.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Zbiór danych} & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\ \hline
AI-GA & 22\,930 & 2\,866 & 2\,866 \\ \hline
XL-Sum (Spanish) & 38\,110 & 4\,763 & 4\,763 \\ \hline
XL-Sum (Nepali) & 5\,808 & 725 & 725 \\ \hline
\end{tabular}
\caption{Liczności podzbiorów dla poszczególnych zadań.}
\label{tab:dataset-sizes}
\end{table}

\subsubsection{Statystyki długości tekstów}
Analiza długości tekstów (liczona w słowach) pozwala na dobór odpowiednich parametrów modelu, takich jak maksymalna długość sekwencji wejściowej.

\begin{itemize}
    \item \textbf{AI-GA}: Średnia długość abstraktu wynosi 189.81 słów.
    \item \textbf{XL-Sum (Spanish)}:
    \begin{itemize}
        \item Średnia długość artykułu: 832.78 słów
        \item Średnia długość streszczenia: 27.93 słów
    \end{itemize}
    \item \textbf{XL-Sum (Nepali)}:
    \begin{itemize}
        \item Średnia długość artykułu: 376.84 słów
        \item Średnia długość streszczenia: 19.82 słów
    \end{itemize}
\end{itemize}

\subsection{Wyniki bazowe (Baselines)}
Przeprowadzono wstępne eksperymenty mające na celu ustanowienie punktów odniesienia (baselines) dla dalszych badań nad metodami PEFT.

\subsubsection{Klasyfikacja (BERT)}
Model BERT-base został poddany pełnemu dostrajaniu (full fine-tuning) przez 1 epokę na zbiorze AI-GA. Uzyskane wyniki wskazują na bardzo wysoką jakość modelu już po krótkim treningu, co sugeruje, że zadanie rozróżniania tekstów generowanych przez AI od ludzkich jest dla tego modelu stosunkowo łatwe przy dostępnej ilości danych.

Wyniki po 1 epoce:
\begin{itemize}
    \item \textbf{Accuracy}: 99.86\%
    \item \textbf{F1-score (macro)}: 99.86\%
    \item \textbf{Eval Loss}: 0.0043
\end{itemize}

\subsubsection{Streszczanie (mT5 Zero-shot)}
Dla zadania streszczania sprawdzono możliwości modelu mT5-small w trybie zero-shot (bez dostrajania) na próbce 200 przykładów dla każdego języka.

Wyniki (metryki ROUGE):
\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Język} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} \\ \hline
Spanish & 0.0419 & 0.0085 & 0.0364 \\ \hline
Nepali & 0.0000 & 0.0000 & 0.0000 \\ \hline
\end{tabular}
\caption{Wyniki zero-shot dla modelu mT5-small.}
\label{tab:mt5-zeroshot}
\end{table}

Niskie wyniki dla języka hiszpańskiego oraz zerowe dla nepalskiego potwierdzają konieczność dostrajania modelu (fine-tuning lub PEFT), aby uzyskać użyteczne streszczenia w tych językach. Model bazowy bez treningu nie jest w stanie generować poprawnych streszczeń w zadanych językach docelowych.

\subsection{Wnioski i wpływ na dalsze prace}

Analiza wstępnych wyników pozwala na sformułowanie kluczowych wniosków.

\begin{enumerate}
    \item \textbf{Wysoka skuteczność w zadaniu klasyfikacji} -- Wynik bliski 100\% dla modelu BERT na zbiorze AI-GA sugeruje, że zadanie rozróżniania tekstów maszynowych od ludzkich w tym zbiorze jest dla modelu relatywnie proste.
    \begin{itemize}
        \item \textit{Implikacja:} W porównaniu metod PEFT różnice w Accuracy mogą być znikome. Dlatego w analizie szczególny nacisk położymy na porównanie wydajności (czas, pamięć) oraz szybkości zbieżności (jak szybko model osiąga wysoki wynik). Rozważymy również scenariusz \textit{low-data} (trening na mniejszym podzbiorze), aby utrudnić zadanie i uwypuklić różnice w efektywności metod.
    \end{itemize}

    \item \textbf{Konieczność adaptacji w zadaniu seq2seq} -- Zerowe lub bliskie zeru wyniki modelu mT5 w trybie zero-shot potwierdzają, że model ten nie posiada natywnej umiejętności streszczania w badanych językach bez dodatkowego treningu.
    \begin{itemize}
        \item \textit{Implikacja:} Jest to dobre środowisko testowe dla metod PEFT, ponieważ będziemy obserwować proces uczenia zadania od zera. Kluczowym pytaniem badawczym staje się, czy metody o małej liczbie parametrów (np. Adaptery, LoRA) będą miały wystarczającą pojemność, aby nauczyć model generowania poprawnego językowo i merytorycznie streszczenia, zwłaszcza dla języka nepalskiego (low-resource).
    \end{itemize}
\end{enumerate}

\section{Finalne wyniki}

\subsection{Baseline}

\subsubsection{Full-Fine-Tune BERT}
W pierwszej fazie eksperymentów przeprowadzono pełne dostrajanie (full fine-tuning) modelu BERT na zbiorze AI-GA przez 3 epoki. Model osiągnął niemal perfekcyjne wyniki, poprawiając rezultaty uzyskane we wstępnej fazie.

Wyniki po 3 epoce:
\begin{itemize}
    \item \textbf{Eval Loss}: 0.0094
    \item \textbf{Accuracy}: 99.90\%
    \item \textbf{F1-score (macro)}: 99.90\%
\end{itemize}

Wysokie wartości metryk Accuracy (odsetek poprawnych predykcji) oraz F1 Macro (średnia arytmetyczna F1-score dla każdej klasy) wskazują na niemal bezbłędne działanie modelu. Zbalansowanie klas w zbiorze danych sprawia, że obie metryki są zbliżone, a stosowanie F1-score nie wnosi istotnych różnic w interpretacji. Wyniki te sugerują, że zadanie klasyfikacji tekstów generowanych przez AI jest dla modelu BERT relatywnie proste. Nie zaobserwowano również oznak wycieku danych między zbiorami treningowym a testowym.

\subsubsection{Full-Fine-Tune mT5 (Hiszpański)}
Dla modelu mT5 w języku hiszpańskim celem było ustalenie górnego pułapu jakości (baseline) poprzez pełne dostrajanie. Ze względu na ograniczenia zasobów obliczeniowych (Google Colab), trening zakończono po 2 epokach, co jednak pozwoliło na uzyskanie solidnego punktu odniesienia.

Wyniki:
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.2453
    \item \textbf{ROUGE-2}: 0.0713
    \item \textbf{ROUGE-L}: 0.1859
\end{itemize}

\subsubsection{Full-Fine-Tune mT5 (Nepalski)}
Następnie przeprowadzono pełny fine-tuning modelu mT5 na zbiorze low-resource (nepalski). Mimo że we wstępnych wynikach (zero-shot) model nie generował poprawnych streszczeń, oczekiwano poprawy po treningu.

Wyniki:
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.0000
    \item \textbf{ROUGE-2}: 0.0000
    \item \textbf{ROUGE-L}: 0.0000
\end{itemize}

Mimo zerowych wartości metryk ROUGE, proces uczenia przebiegał poprawnie z punktu widzenia optymalizacji funkcja straty systematycznie malała, a model generował sekwencje o niezerowej długości. Wykres ten przedstawiony jest na obrazku poniżej:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{full-mt5-ne.png}
    \caption{Wykres funkcji straty podczas treningu modelu mT5 na zbiorze nepalskim}
\end{figure}
Sugeruje to, że model nie uległ degeneracji. Zaobserwowana rozbieżność między spadkiem Loss a zerowym ROUGE wskazuje na potencjalne problemy z ewaluacją. Prawdopodobnym wyjaśnieniem jest ograniczona skuteczność standardowej implementacji metryki ROUGE dla języków o alfabecie innym niż łaciński (problem z tokenizacją) lub specyfika morfologiczna języka nepalskiego.

\subsubsection{Full-Fine-Tuning-Downsamples mT5 (Hiszpański)}
Aby zweryfikować hipotezę, czy mała ilość danych (jak w przypadku nepalskiego) jest wystarczająca do nauczenia modelu poprawnego streszczania, przeprowadzono dodatkowy eksperyment. Zbiór treningowy dla języka hiszpańskiego został ograniczony do liczności zbioru nepalskiego.

Wyniki:
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.2275
    \item \textbf{ROUGE-2}: 0.0501
    \item \textbf{ROUGE-L}: 0.1722
\end{itemize}

Wyniki są tylko nieznacznie niższe od uzyskanych na pełnym zbiorze. Potwierdza to, że nawet ograniczona ilość danych jest wystarczająca do efektywnego treningu, o ile dane są dobrej jakości.

\subsection{Klasyfikacja (BERT)}

\subsubsection{LoRA}
Dla metody LoRA zastosowano następującą konfigurację:
\begin{minted}{python}
peft_config_bert_lora = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    target_modules=["query", "value"]
)
\end{minted}

Parametr $r$ (rank) w metodzie LoRA determinuje wymiar macierzy niskiego rzędu, które służą do aproksymacji zmian wag modelu. Wybór wartości $r$ jest kluczową decyzją projektową.

W pracy wprowadzającej metodę LoRA (Hu et al., 2021)\footnote{Edward J. Hu et al., \textit{LoRA: Low-Rank Adaptation of Large Language Models}, 2021. Rozdział 7.2. Dostępne pod adresem: \url{https://doi.org/10.48550/arXiv.2106.09685}}, autorzy wykazują, że dla standardowych zadań w języku angielskim wystarczające są bardzo niskie wartości rzędu $r=4$ lub $r=8$. Jednakże, jak zauważają w przypisie nr 6, w sytuacji gdy język zadania docelowego różni się od języka pre-treningu (co ma miejsce w naszej części eksperymentów z językiem hiszpańskim), małe wartości $r$ mogą nie wystarczyć do skutecznej adaptacji. Z tego względu, oraz dla zachowania spójności metodologicznej w całym projekcie, zdecydowaliśmy się na wartość $r=16$. Parametr $\alpha=32$ (skalowanie) został dobrany proporcjonalnie do $r$ (zazwyczaj $\alpha \approx 2r$), co pomaga w stabilizacji treningu. Wybór modułów \texttt{query} i \texttt{value} w mechanizmie atencji jest standardową praktyką dla modeli BERT, pozwalającą na efektywne dostrajanie reprezentacji semantycznych.

Wyniki:
\begin{itemize}
    \item \textbf{Accuracy}: 99.48\%
\end{itemize}

Wynik jest tylko nieznacznie gorszy od pełnego dostrajania (99.90\%), natomiast czas treningu uległ skróceniu (2417s vs 2708s, ok. 12\% szybciej). Wskazuje to na wysoką efektywność metody LoRA.

\subsubsection{Prefix Tuning}
Konfiguracja dla metody Prefix Tuning:
\begin{minted}{python}
peft_config_bert_prefix = PrefixTuningConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    num_virtual_tokens=20
)
\end{minted}

Parametr \texttt{inference\_mode=False} definiuje tryb pracy modelu jako treningowy, co jest niezbędne do aktualizacji parametrów prefiksu. Liczba wirtualnych tokenów została ustalona na 20. Jest to wartość często stosowana w literaturze jako punkt wyjścia, zbyt mała liczba może nie wystarczyć do zakodowania zadania, a zbyt duża utrudnia optymalizację.

Wyniki:
\begin{itemize}
    \item \textbf{Accuracy}: 77.11\%
\end{itemize}

Mimo najkrótszego czasu treningu (2294s), metoda ta osiągnęła znacząco gorsze wyniki niż pozostałe podejścia. Sugeruje to, że dla tego konkretnego zadania i modelu, Prefix Tuning ma zbyt małą pojemność lub jest trudniejszy w optymalizacji.

\subsubsection{Adapter (Houlsby)}
Zastosowano architekturę adapterów typu Houlsby ponieważ architektura ta jest uznawana za standard w dziedzinie adapterów. W przeciwieństwie do nowszych rozwiązań (np. Pfeiffer), umieszcza ona moduły adapterów po obu podwarstwach (Attention i Feed-Forward) w każdym bloku transformera. Choć zwiększa to liczbę parametrów, zapewnia większą elastyczność i pozwala uniknąć problemów z brakiem adapterów w ostatnich warstwach.\footnote{Informacje znalezione pod adresem: \url{https://github.com/adapter-hub/adapters/issues/168}}

Wyniki:
\begin{itemize}
    \item \textbf{Accuracy}: 99.37\%
\end{itemize}

Wyniki są bardzo zbliżone do metody LoRA, a czas treningu (2453s) jest porównywalny. Stanowi to doskonałą alternatywę dla LoRA, oferując wysoką jakość przy zredukowanych kosztach.

\subsection{Streszczanie (mT5) - Hiszapńsi pełny}

\subsubsection{LoRA}
Konfiguracja LoRA dla modelu mT5:
\begin{minted}{python}
peft_config_mt5_lora = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    inference_mode=False,
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    target_modules=["q", "v"]
)
\end{minted}

Parametry dobrano analogicznie do modelu BERT, dostosowując nazwy modułów (\texttt{q}, \texttt{v}) do specyfiki architektury T5.

Wyniki (na pełnym zbiorze hiszpańskim):
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.2477
    \item \textbf{ROUGE-2}: 0.0631
    \item \textbf{ROUGE-L}: 0.1889
\end{itemize}

Wyniki są porównywalne z pełnym fine-tuningiem (szczególnie w wariancie downsampled), co potwierdza skuteczność LoRA w zadaniach generatywnych.

\subsubsection{Prefix Tuning}
Konfiguracja Prefix Tuning:
\begin{minted}{python}
peft_config_mt5_prefix = PrefixTuningConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    inference_mode=False,
    num_virtual_tokens=30
)
\end{minted}

Zwiększono liczbę wirtualnych tokenów do 30, aby zwiększyć pojemność modelu w bardziej złożonym zadaniu seq2seq.

Wyniki:
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.1384
    \item \textbf{ROUGE-2}: 0.0224
    \item \textbf{ROUGE-L}: 0.1197
\end{itemize}

Podobnie jak w przypadku BERT, wyniki są wyraźnie słabsze od konkurencji. Czas treningu (6140s) był jednak znacznie krótszy niż dla LoRA (8819s), co potwierdza korelację między redukcją parametrów a szybkością uczenia, niestety w tym przypadku kosztem jakości.

\subsubsection{Adapter (Pfeiffer)}
Zastosowano architekturę adapterów typu Pfeiffer ponieważ w modelach seq2seq, takich jak mT5, architektura Pfeiffer jest często preferowana ze względu na większą efektywność (jeden adapter na blok). Pozwala to na zachowanie dobrego balansu między wydajnością a kosztami obliczeniowymi.

Wyniki:
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.2546
    \item \textbf{ROUGE-2}: 0.0662
    \item \textbf{ROUGE-L}: 0.1911
\end{itemize}

Metoda ta, mimo najdłuższego czasu treningu (9454s), osiągnęła najlepsze wyniki, przewyższając w metrykach ROUGE-1 i ROUGE-L nawet pełny fine-tuning (baseline). Wskazuje to, że adaptery Pfeiffera są niezwykle skuteczne w adaptacji modelu mT5 do nowych języków.


\subsection{Streszczanie (mT5) - Hiszpański ograniczony}

Eksperyment ten został przeprowadzony jako alternatywa dla badania na języku nepalskim. Ze względu na problemy techniczne związane z ewaluacją modelu dla języka nepalskiego (prawdopodobnie wynikające z niekompatybilności standardowych implementacji metryk ROUGE z alfabetem innym niż łaciński przy naszej tokenizacji), nie uzyskaliśmy wiarygodnych wyników dla tego języka (ROUGE = 0.0), mimo spadku funkcji straty. Aby mimo to zbadać zachowanie metod PEFT w warunkach \textit{low-resource}, zdecydowaliśmy się na ograniczenie zbioru treningowego języka hiszpańskiego do liczności zbioru nepalskiego (5\,808 przykładów). Zbiory walidacyjny i testowy pozostały bez zmian (pełny wymiar dla języka hiszpańskiego), co pozwala na bezpośrednie porównanie wyników z modelami trenowanymi na pełnym zbiorze.

\subsubsection{LoRA}
Konfiguracja LoRA pozostała taka sama jak w eksperymencie na pełnym zbiorze ($r=16, \alpha=32$).

Wyniki:
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.2347
    \item \textbf{ROUGE-2}: 0.0542
    \item \textbf{ROUGE-L}: 0.1769
    \item \textbf{Eval Loss}: 3.0764
\end{itemize}

Jest to wynik niezwykle interesujący. Porównując go z pełnym dostrajaniem na ograniczonym zbiorze (ROUGE-1: 0.2275), metoda LoRA osiągnęła \textbf{lepsze} wyniki. Sugeruje to, że w warunkach ograniczonej ilości danych, metody PEFT (dzięki mniejszej liczbie parametrów) są mniej podatne na przeuczenie (overfitting) i mogą generalizować lepiej niż pełny fine-tuning. Czas treningu wyniósł 4553s.

\subsubsection{Prefix Tuning}
Wyniki:
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.1015
    \item \textbf{ROUGE-2}: 0.0101
    \item \textbf{ROUGE-L}: 0.0831
    \item \textbf{Eval Loss}: 6.3560
\end{itemize}

Prefix Tuning w warunkach małej ilości danych poradził sobie najgorzej, osiągając wyniki o połowę gorsze od pozostałych metod. Potwierdza to obserwację z pełnego zbioru, że metoda ta (w standardowej konfiguracji) może być mniej stabilna lub wymagać dłuższego strojenia hiperparametrów dla modelu mT5. Czas treningu był najkrótszy (2853s).

\subsubsection{Adapter (Pfeiffer)}
Wyniki:
\begin{itemize}
    \item \textbf{ROUGE-1}: 0.2298
    \item \textbf{ROUGE-2}: 0.0548
    \item \textbf{ROUGE-L}: 0.1756
    \item \textbf{Eval Loss}: 2.9415
\end{itemize}

Adaptery Pfeiffera osiągnęły wyniki bardzo zbliżone do metody LoRA i również (nieznacznie) przewyższyły pełny fine-tuning na tym samym podzbiorze danych. Potwierdza to hipotezę o przewadze metod PEFT w scenariuszach \textit{low-resource}. Czas treningu wyniósł 5086s.

\section{Podsumowanie}

Przeprowadzone w ramach projektu eksperymenty pozwoliły na dogłębną analizę efektywności metod \\
Parameter-Efficient Fine-Tuning w porównaniu do pełnego dostrajania modeli (Full Fine-Tuning). Badania objęły dwa rodzaje zadań: klasyfikację (BERT) oraz sekwencyjne generowanie tekstu (mT5), w tym symulowany scenariusz \textit{low-resource}.

Kluczowe wnioski płynące z badań:

\begin{enumerate}
    \item \textbf{Efektywność w zadaniach klasyfikacyjnych:} Metody takie jak LoRA i Adaptery (Houlsby) są w stanie w pełni dorównać jakością pełnemu dostrajaniu (Accuracy $>99\%$ na zbiorze AI-GA), oferując przy tym redukcję kosztów obliczeniowych i pamięciowych.
    
    \item \textbf{Przewaga PEFT w scenariuszach low-resource:} Jednym z najważniejszych odkryć projektu jest zachowanie metod PEFT w warunkach ograniczonej ilości danych (hiszpański ograniczony). Zarówno LoRA, jak i Adaptery osiągnęły w tym scenariuszu wyniki \textbf{lepsze} niż pełny fine-tuning (ROUGE-1: $\sim$0.235 vs 0.228). Wskazuje to na właściwości regularyzacyjne PEFT - zamrożenie większości parametrów modelu zapobiega przeuczeniu się na małym zbiorze treningowym, co pozwala na lepszą generalizację.
    
    \item \textbf{Jakość w zadaniach generatywnych:} W zadaniu streszczania na pełnym zbiorze, Adaptery (Pfeiffer) okazały się bezkonkurencyjne, przewyższając nawet baseline. LoRA uzyskała wyniki zbliżone do pełnego treningu.
    
    \item \textbf{Słabsze wyniki Prefix Tuning:} W naszych eksperymentach metoda ta konsekwentnie osiągała gorsze rezultaty niż konkurencja, zarówno dla modelu BERT, jak i mT5. Może to sugerować większą wrażliwość tej metody na dobór hiperparametrów lub specyfikę testowanych zbiorów danych.
\end{enumerate}

Podsumowując, metody PEFT, w szczególności LoRA i Adaptery, stanowią nie tylko oszczędną alternatywę dla klasycznego treningu, ale w pewnych warunkach (ograniczone dane, ryzyko overfittingu) mogą być podejściem preferowanym ze względu na wyższą jakość końcową modelu.


\vspace{30pt}

Link do repozytorium z kodem: \url{https://github.com/Tombiczek/NLP-Final-Project}

Link do repozytorium wandb: \url{https://wandb.ai/tombik-warsaw-university-of-technology/nlp-project-peft/table?nw=nwusertombik}
\end{document}
